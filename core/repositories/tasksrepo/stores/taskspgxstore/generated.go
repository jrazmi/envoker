// Code generated by pgxstores. DO NOT EDIT.
// This file is ALWAYS REGENERATED - do not modify.
// For custom queries, see store.go which embeds the generated store.

package taskspgxstore

import (
	"bytes"
	"context"
	"errors"
	"fmt"
	"strings"
	"time"

	"github.com/jackc/pgx/v5"
	"github.com/jrazmi/envoker/core/repositories/tasksrepo"
	"github.com/jrazmi/envoker/core/scaffolding/fop"
	"github.com/jrazmi/envoker/infrastructure/postgresdb"
	"github.com/jrazmi/envoker/sdk/logger"
)

// ========================================
// GENERATED STORE
// ========================================

// GeneratedStore provides default implementations for all Task SQL operations.
// Embed this struct in your custom Store (in store.go) to inherit default behavior
// that you can selectively override.
type GeneratedStore struct {
	log  *logger.Logger
	pool *postgresdb.Pool
}

// Create inserts a new Task
func (s *GeneratedStore) Create(ctx context.Context, input tasksrepo.CreateTask) (tasksrepo.Task, error) {
	// PK is in Create struct - use value from input
	query := `INSERT INTO public.tasks (task_id, processing_status, task_type, metadata, priority, max_retries, retry_count, error_message, processing_time_ms, last_run_at) VALUES (@task_id, @processing_status, @task_type, @metadata, @priority, @max_retries, @retry_count, @error_message, @processing_time_ms, @last_run_at) RETURNING task_id, processing_status, created_at, updated_at, task_type, metadata, priority, max_retries, retry_count, error_message, processing_time_ms, last_run_at`

	args := pgx.NamedArgs{
		"task_id":            input.TaskId,
		"processing_status":  input.ProcessingStatus,
		"task_type":          input.TaskType,
		"metadata":           input.Metadata,
		"priority":           input.Priority,
		"max_retries":        input.MaxRetries,
		"retry_count":        input.RetryCount,
		"error_message":      input.ErrorMessage,
		"processing_time_ms": input.ProcessingTimeMs,
		"last_run_at":        input.LastRunAt,
	}

	rows, err := s.pool.Query(ctx, query, args)
	if err != nil {
		return tasksrepo.Task{}, postgresdb.HandlePgError(err)
	}
	defer rows.Close()

	record, err := pgx.CollectOneRow(rows, pgx.RowToStructByName[tasksrepo.Task])
	if err != nil {
		return tasksrepo.Task{}, postgresdb.HandlePgError(err)
	}

	return record, nil
}

// Get retrieves a single Task by ID
func (s *GeneratedStore) Get(ctx context.Context, taskId string) (tasksrepo.Task, error) {
	query := `SELECT task_id, processing_status, created_at, updated_at, task_type, metadata, priority, max_retries, retry_count, error_message, processing_time_ms, last_run_at FROM public.tasks WHERE task_id = @taskId`

	args := pgx.NamedArgs{
		"taskId": taskId,
	}

	rows, err := s.pool.Query(ctx, query, args)
	if err != nil {
		return tasksrepo.Task{}, postgresdb.HandlePgError(err)
	}
	defer rows.Close()

	record, err := pgx.CollectOneRow(rows, pgx.RowToStructByName[tasksrepo.Task])
	if err != nil {
		if errors.Is(err, pgx.ErrNoRows) {
			return tasksrepo.Task{}, fmt.Errorf("Task not found")
		}
		return tasksrepo.Task{}, postgresdb.HandlePgError(err)
	}

	return record, nil
}

// Update modifies an existing Task
func (s *GeneratedStore) Update(ctx context.Context, taskId string, input tasksrepo.UpdateTask) error {
	buf := bytes.NewBufferString("UPDATE public.tasks SET ")
	args := pgx.NamedArgs{
		"taskId": taskId,
	}
	var fields []string
	if input.ProcessingStatus != nil {
		fields = append(fields, "processing_status = @processing_status")
		args["processing_status"] = *input.ProcessingStatus
	}
	if input.TaskType != nil {
		fields = append(fields, "task_type = @task_type")
		args["task_type"] = *input.TaskType
	}
	if input.Metadata != nil {
		fields = append(fields, "metadata = @metadata")
		args["metadata"] = *input.Metadata
	}
	if input.Priority != nil {
		fields = append(fields, "priority = @priority")
		args["priority"] = *input.Priority
	}
	if input.MaxRetries != nil {
		fields = append(fields, "max_retries = @max_retries")
		args["max_retries"] = *input.MaxRetries
	}
	if input.RetryCount != nil {
		fields = append(fields, "retry_count = @retry_count")
		args["retry_count"] = *input.RetryCount
	}
	if input.ErrorMessage != nil {
		fields = append(fields, "error_message = @error_message")
		args["error_message"] = *input.ErrorMessage
	}
	if input.ProcessingTimeMs != nil {
		fields = append(fields, "processing_time_ms = @processing_time_ms")
		args["processing_time_ms"] = *input.ProcessingTimeMs
	}
	if input.LastRunAt != nil {
		fields = append(fields, "last_run_at = @last_run_at")
		args["last_run_at"] = *input.LastRunAt
	}

	// Always update the updated_at field
	now := time.Now().UTC()
	if input.UpdatedAt != nil {
		args["updated_at"] = *input.UpdatedAt
	} else {
		args["updated_at"] = now
	}
	fields = append(fields, "updated_at = @updated_at")

	// If no fields to update besides updated_at, return early
	if len(fields) == 1 {
		return fmt.Errorf("no fields to update")
	}

	// Join fields and complete the query
	buf.WriteString(strings.Join(fields, ", "))
	buf.WriteString(" WHERE task_id = @taskId")

	query := buf.String()
	s.log.DebugContext(ctx, "update Task", "query", query, "taskId", taskId)

	result, err := s.pool.Exec(ctx, query, args)
	if err != nil {
		return postgresdb.HandlePgError(err)
	}

	if result.RowsAffected() == 0 {
		return fmt.Errorf("Task not found")
	}

	return nil
}

// Delete removes a Task by ID
func (s *GeneratedStore) Delete(ctx context.Context, taskId string) error {
	query := `DELETE FROM public.tasks WHERE task_id = @taskId`

	args := pgx.NamedArgs{
		"taskId": taskId,
	}

	result, err := s.pool.Exec(ctx, query, args)
	if err != nil {
		return postgresdb.HandlePgError(err)
	}

	if result.RowsAffected() == 0 {
		return fmt.Errorf("Task not found")
	}

	return nil
}

// List retrieves Task records with filtering, ordering, and cursor pagination
func (s *GeneratedStore) List(ctx context.Context, filter tasksrepo.TaskFilter, orderBy fop.By, page fop.PageStringCursor, forPrevious bool) ([]tasksrepo.Task, error) {
	data := pgx.NamedArgs{}

	// Start building the query
	buf := bytes.NewBufferString(`
		SELECT
			task_id,
			processing_status,
			created_at,
			updated_at,
			task_type,
			metadata,
			priority,
			max_retries,
			retry_count,
			error_message,
			processing_time_ms,
			last_run_at
		FROM
			public.tasks`)

	// Apply filters
	s.applyFilter(filter, data, buf)

	// Setup configuration for string cursor pagination
	cursorConfig := postgresdb.StringCursorConfig{
		Cursor:     page.Cursor,
		OrderField: orderByFields[orderBy.Field],
		PKField:    "task_id",
		TableName:  "tasks",
		Direction:  orderBy.Direction,
		Limit:      page.Limit,
	}

	// Apply cursor pagination
	if page.Cursor != "" {
		err := postgresdb.ApplyStringCursorPagination[time.Time](buf, data, cursorConfig, forPrevious)
		if err != nil {
			return nil, fmt.Errorf("cursorpagination: %s", err)
		}
	}

	// Add ordering
	err := postgresdb.AddOrderByClause(buf, cursorConfig.OrderField, cursorConfig.PKField, cursorConfig.Direction, forPrevious)
	if err != nil {
		return nil, fmt.Errorf("order: %w", err)
	}

	// Add limit
	postgresdb.AddLimitClause(cursorConfig.Limit, data, buf)

	// Execute the query
	query := buf.String()
	s.log.DebugContext(ctx, "list Task", "query", query)

	rows, err := s.pool.Query(ctx, query, data)
	if err != nil {
		return nil, postgresdb.HandlePgError(err)
	}
	defer rows.Close()

	entities, err := pgx.CollectRows(rows, pgx.RowToStructByName[tasksrepo.Task])
	if err != nil {
		return nil, postgresdb.HandlePgError(err)
	}

	// If we were getting previous page, reverse the results back to correct order
	if forPrevious && len(entities) > 0 {
		for i, j := 0, len(entities)-1; i < j; i, j = i+1, j-1 {
			entities[i], entities[j] = entities[j], entities[i]
		}
	}

	return entities, nil
}

// ========================================
// FILTER & ORDERING HELPERS
// ========================================

// orderByFields maps repository field names to database column names
var orderByFields = map[string]string{
	tasksrepo.OrderByPK:               "task_id",
	tasksrepo.OrderByCreatedAt:        "created_at",
	tasksrepo.OrderByUpdatedAt:        "updated_at",
	tasksrepo.OrderByProcessingStatus: "processing_status",
	tasksrepo.OrderByTaskType:         "task_type",
	tasksrepo.OrderByMetadata:         "metadata",
	tasksrepo.OrderByPriority:         "priority",
	tasksrepo.OrderByMaxRetries:       "max_retries",
	tasksrepo.OrderByRetryCount:       "retry_count",
	tasksrepo.OrderByErrorMessage:     "error_message",
	tasksrepo.OrderByProcessingTimeMs: "processing_time_ms",
	tasksrepo.OrderByLastRunAt:        "last_run_at",
}

// applyFilter applies query filters to the SQL query
func (s *GeneratedStore) applyFilter(filter tasksrepo.TaskFilter, data pgx.NamedArgs, buf *bytes.Buffer) {
	var conditions []string
	// Filter by processing_status
	if filter.ProcessingStatus != nil {
		conditions = append(conditions, "processing_status = @processingStatus")
		data["processingStatus"] = *filter.ProcessingStatus
	}
	// Filter by created_at - before
	if filter.CreatedAtBefore != nil {
		conditions = append(conditions, "created_at < @createdAt_before")
		data["createdAt_before"] = *filter.CreatedAtBefore
	}

	// Filter by created_at - after
	if filter.CreatedAtAfter != nil {
		conditions = append(conditions, "created_at > @createdAt_after")
		data["createdAt_after"] = *filter.CreatedAtAfter
	}
	// Filter by updated_at - before
	if filter.UpdatedAtBefore != nil {
		conditions = append(conditions, "updated_at < @updatedAt_before")
		data["updatedAt_before"] = *filter.UpdatedAtBefore
	}

	// Filter by updated_at - after
	if filter.UpdatedAtAfter != nil {
		conditions = append(conditions, "updated_at > @updatedAt_after")
		data["updatedAt_after"] = *filter.UpdatedAtAfter
	}
	// Filter by task_type
	if filter.TaskType != nil {
		conditions = append(conditions, "task_type = @taskType")
		data["taskType"] = *filter.TaskType
	}
	// Filter by priority
	if filter.Priority != nil {
		conditions = append(conditions, "priority = @priority")
		data["priority"] = *filter.Priority
	}
	// Filter by max_retries
	if filter.MaxRetries != nil {
		conditions = append(conditions, "max_retries = @maxRetries")
		data["maxRetries"] = *filter.MaxRetries
	}
	// Filter by retry_count
	if filter.RetryCount != nil {
		conditions = append(conditions, "retry_count = @retryCount")
		data["retryCount"] = *filter.RetryCount
	}
	// Filter by error_message
	if filter.ErrorMessage != nil {
		conditions = append(conditions, "error_message = @errorMessage")
		data["errorMessage"] = *filter.ErrorMessage
	}
	// Filter by processing_time_ms
	if filter.ProcessingTimeMs != nil {
		conditions = append(conditions, "processing_time_ms = @processingTimeMs")
		data["processingTimeMs"] = *filter.ProcessingTimeMs
	}
	// Filter by last_run_at
	if filter.LastRunAt != nil {
		conditions = append(conditions, "last_run_at = @lastRunAt")
		data["lastRunAt"] = *filter.LastRunAt
	}

	// Search term across text fields
	if filter.SearchTerm != nil && *filter.SearchTerm != "" {
		searchPattern := "%" + *filter.SearchTerm + "%"
		searchConditions := []string{}
		searchConditions = append(searchConditions, "task_id ILIKE @search_term")
		searchConditions = append(searchConditions, "processing_status ILIKE @search_term")
		searchConditions = append(searchConditions, "task_type ILIKE @search_term")
		searchConditions = append(searchConditions, "error_message ILIKE @search_term")
		if len(searchConditions) > 0 {
			conditions = append(conditions, "("+strings.Join(searchConditions, " OR ")+")")
			data["search_term"] = searchPattern
		}
	}

	// Apply conditions if any exist
	if len(conditions) > 0 {
		buf.WriteString(" WHERE ")
		buf.WriteString(strings.Join(conditions, " AND "))
	}
}
